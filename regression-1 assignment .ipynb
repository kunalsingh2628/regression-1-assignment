{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9121d4e8-1444-4d18-b9cd-fb6d9030bc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9786d325-cc93-4e0b-b800-ab46e6e7a411",
   "metadata": {},
   "outputs": [],
   "source": [
    "Simple linear regression is a statistical method used to model the relationship between a single independent variable (predictor) and a dependent variable (response). The goal is to find a linear equation that best predicts the dependent variable based on the independent variable. The equation takes the form:\n",
    "\n",
    "�\n",
    "=\n",
    "�\n",
    "0\n",
    "+\n",
    "�\n",
    "1\n",
    "�\n",
    "+\n",
    "�\n",
    "Y=b \n",
    "0\n",
    "​\n",
    " +b \n",
    "1\n",
    "​\n",
    " X+ε\n",
    "\n",
    "�\n",
    "Y is the dependent variable.\n",
    "�\n",
    "X is the independent variable.\n",
    "�\n",
    "0\n",
    "b \n",
    "0\n",
    "​\n",
    "  is the y-intercept (constant term).\n",
    "�\n",
    "1\n",
    "b \n",
    "1\n",
    "​\n",
    "  is the slope of the line, representing the change in \n",
    "�\n",
    "Y for a one-unit change in \n",
    "�\n",
    "X.\n",
    "�\n",
    "ε is the error term, representing the unobserved factors that affect \n",
    "�\n",
    "Y but are not included in the model.\n",
    "Example of Simple Linear Regression:\n",
    "Suppose we want to predict a person's weight (\n",
    "�\n",
    "Y) based on their height (\n",
    "�\n",
    "X). The simple linear regression equation would be:\n",
    "\n",
    "Weight\n",
    "=\n",
    "�\n",
    "0\n",
    "+\n",
    "�\n",
    "1\n",
    "×\n",
    "Height\n",
    "+\n",
    "�\n",
    "Weight=b \n",
    "0\n",
    "​\n",
    " +b \n",
    "1\n",
    "​\n",
    " ×Height+ε\n",
    "\n",
    "Here, \n",
    "�\n",
    "0\n",
    "b \n",
    "0\n",
    "​\n",
    "  is the intercept, \n",
    "�\n",
    "1\n",
    "b \n",
    "1\n",
    "​\n",
    "  is the slope, and \n",
    "�\n",
    "ε represents the error term.\n",
    "\n",
    "Multiple Linear Regression:\n",
    "Multiple linear regression extends the concept of simple linear regression to multiple independent variables. It models the relationship between a dependent variable and two or more independent variables. The equation takes the form:\n",
    "\n",
    "�\n",
    "=\n",
    "�\n",
    "0\n",
    "+\n",
    "�\n",
    "1\n",
    "�\n",
    "1\n",
    "+\n",
    "�\n",
    "2\n",
    "�\n",
    "2\n",
    "+\n",
    "…\n",
    "+\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "Y=b \n",
    "0\n",
    "​\n",
    " +b \n",
    "1\n",
    "​\n",
    " X \n",
    "1\n",
    "​\n",
    " +b \n",
    "2\n",
    "​\n",
    " X \n",
    "2\n",
    "​\n",
    " +…+b \n",
    "n\n",
    "​\n",
    " X \n",
    "n\n",
    "​\n",
    " +ε\n",
    "\n",
    "�\n",
    "Y is the dependent variable.\n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ",\n",
    "…\n",
    ",\n",
    "�\n",
    "�\n",
    "X \n",
    "1\n",
    "​\n",
    " ,X \n",
    "2\n",
    "​\n",
    " ,…,X \n",
    "n\n",
    "​\n",
    "  are the independent variables.\n",
    "�\n",
    "0\n",
    "b \n",
    "0\n",
    "​\n",
    "  is the y-intercept.\n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ",\n",
    "…\n",
    ",\n",
    "�\n",
    "�\n",
    "b \n",
    "1\n",
    "​\n",
    " ,b \n",
    "2\n",
    "​\n",
    " ,…,b \n",
    "n\n",
    "​\n",
    "  are the slopes for the corresponding independent variables.\n",
    "�\n",
    "ε is the error term.\n",
    "Example of Multiple Linear Regression:\n",
    "Suppose we want to predict a house's price (\n",
    "�\n",
    "Y) based on its size (\n",
    "�\n",
    "1\n",
    "X \n",
    "1\n",
    "​\n",
    " ), number of bedrooms (\n",
    "�\n",
    "2\n",
    "X \n",
    "2\n",
    "​\n",
    " ), and distance to the city center (\n",
    "�\n",
    "3\n",
    "X \n",
    "3\n",
    "​\n",
    " ). The multiple linear regression equation would be:\n",
    "\n",
    "Price\n",
    "=\n",
    "�\n",
    "0\n",
    "+\n",
    "�\n",
    "1\n",
    "×\n",
    "Size\n",
    "+\n",
    "�\n",
    "2\n",
    "×\n",
    "Bedrooms\n",
    "+\n",
    "�\n",
    "3\n",
    "×\n",
    "Distance\n",
    "+\n",
    "�\n",
    "Price=b \n",
    "0\n",
    "​\n",
    " +b \n",
    "1\n",
    "​\n",
    " ×Size+b \n",
    "2\n",
    "​\n",
    " ×Bedrooms+b \n",
    "3\n",
    "​\n",
    " ×Distance+ε\n",
    "\n",
    "Here, \n",
    "�\n",
    "0\n",
    "b \n",
    "0\n",
    "​\n",
    "  is the intercept, and \n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ",\n",
    "b \n",
    "1\n",
    "​\n",
    " ,b \n",
    "2\n",
    "​\n",
    " , and \n",
    "�\n",
    "3\n",
    "b \n",
    "3\n",
    "​\n",
    "  are the slopes for size, bedrooms, and distance, respectively. \n",
    "�\n",
    "ε represents the error term.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e61aa66-70f5-4ce2-8b2c-7e0254ded5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16118002-0a19-4605-9bed-436ab18ba8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Linear regression relies on several assumptions to be valid. It's important to check these assumptions to ensure the reliability of the regression model. The main assumptions of linear regression are:\n",
    "\n",
    "Linearity: The relationship between the independent and dependent variables is assumed to be linear. You can check this assumption by creating scatter plots of the variables to see if a straight line can reasonably describe their relationship.\n",
    "\n",
    "Independence of Errors: The errors (residuals) should be independent, meaning that the value of one residual should not predict the value of another. You can check this by examining a plot of residuals against the predicted values. There should be no clear pattern or trend in the residuals.\n",
    "\n",
    "Homoscedasticity: Homoscedasticity means that the variance of the residuals is constant across all levels of the independent variables. A plot of residuals against predicted values should show a constant spread of points, with no funnel-like shape.\n",
    "\n",
    "Normality of Residuals: The residuals should be approximately normally distributed. This can be checked using histograms, Q-Q plots, or statistical tests for normality. In large samples, the Central Limit Theorem often makes this assumption less critical.\n",
    "\n",
    "No or Little Multicollinearity: In multiple linear regression, the independent variables should not be highly correlated with each other. High multicollinearity can lead to inflated standard errors of the regression coefficients, making it difficult to identify the individual contribution of each variable.\n",
    "\n",
    "No Perfect Multicollinearity: Perfect multicollinearity, where one independent variable is a perfect linear function of another, must be avoided. This typically arises when variables are duplicated or almost duplicated.\n",
    "\n",
    "Checking Assumptions:\n",
    "\n",
    "Residual Analysis: Examine residuals by plotting them against predicted values. Look for patterns or trends that violate assumptions.\n",
    "\n",
    "Normality Tests: Use statistical tests like the Shapiro-Wilk test or visual inspections (histograms, Q-Q plots) to assess normality of residuals.\n",
    "\n",
    "Homoscedasticity: Plot residuals against predicted values and check for a consistent spread of points.\n",
    "\n",
    "Linearity: Create scatter plots of the variables and check for a linear relationship.\n",
    "\n",
    "VIF (Variance Inflation Factor): Calculate VIF to check for multicollinearity among independent variables.\n",
    "\n",
    "Durbin-Watson Test: Assess the independence of residuals using the Durbin-Watson statistic. A value around 2 indicates no significant autocorrelation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bb2fdd-0109-4130-a8a4-da1bff8ecd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429828fc-33da-4447-a888-7cbc00d775cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "In a linear regression model, the slope and intercept have specific interpretations in the context of the variables being modeled.\n",
    "\n",
    "Intercept (\n",
    "�\n",
    "0\n",
    "b \n",
    "0\n",
    "​\n",
    " ):\n",
    "\n",
    "The intercept represents the estimated value of the dependent variable when all independent variables are set to zero.\n",
    "In many cases, the intercept may not have a meaningful interpretation, especially if setting all variables to zero is not realistic or meaningful in the context of the data.\n",
    "It serves as a starting point for the regression line.\n",
    "Slope (\n",
    "�\n",
    "1\n",
    "b \n",
    "1\n",
    "​\n",
    " ):\n",
    "\n",
    "The slope represents the change in the dependent variable for a one-unit change in the independent variable, assuming all other variables are held constant.\n",
    "It quantifies the strength and direction of the linear relationship between the independent and dependent variables.\n",
    "Example:\n",
    "Let's consider a real-world scenario where we want to predict a student's score on a final exam based on the number of hours they spent studying. The linear regression equation is:\n",
    "\n",
    "Exam Score\n",
    "=\n",
    "�\n",
    "0\n",
    "+\n",
    "�\n",
    "1\n",
    "×\n",
    "Hours Studied\n",
    "+\n",
    "�\n",
    "Exam Score=b \n",
    "0\n",
    "​\n",
    " +b \n",
    "1\n",
    "​\n",
    " ×Hours Studied+ε\n",
    "\n",
    "�\n",
    "0\n",
    "b \n",
    "0\n",
    "​\n",
    "  (Intercept): If a student studied for zero hours, the predicted exam score would be the intercept. However, interpreting this may not be meaningful in this context because it implies a scenario where a student didn't study at all.\n",
    "\n",
    "�\n",
    "1\n",
    "b \n",
    "1\n",
    "​\n",
    "  (Slope): The slope represents the change in the exam score for a one-hour increase in study time, assuming all other factors remain constant. For example, if \n",
    "�\n",
    "1\n",
    "=\n",
    "5\n",
    "b \n",
    "1\n",
    "​\n",
    " =5, it means that, on average, the exam score is expected to increase by 5 points for each additional hour of study.\n",
    "\n",
    "So, if the regression equation is \n",
    "Exam Score\n",
    "=\n",
    "50\n",
    "+\n",
    "5\n",
    "×\n",
    "Hours Studied\n",
    "+\n",
    "�\n",
    "Exam Score=50+5×Hours Studied+ε, it implies that students, on average, start with a score of 50 when they haven't studied, and for each additional hour of study, their expected exam score increases by 5 points, assuming other factors are constant.\n",
    "\n",
    "It's crucial to interpret the coefficients in the context of the specific variables and data being analyzed. Also, keep in mind that correlation does not imply causation, and the interpretation should be cautious about inferring a causal relationship based solely on the regression coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e119b3-3d5c-4f1b-b193-11f3112e25bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df50966-78e6-44df-b346-75cbf4cada25",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient descent is an optimization algorithm used to minimize the cost function in the context of machine learning and optimization problems. The goal of the algorithm is to find the minimum of a function by iteratively moving in the direction of steepest decrease of the function.\n",
    "\n",
    "Key Concepts:\n",
    "\n",
    "Cost Function:\n",
    "\n",
    "In machine learning, the cost function (also known as the loss or objective function) measures the difference between the predicted values of the model and the actual values.\n",
    "The goal is to minimize this cost function to improve the accuracy of the model.\n",
    "Parameters (Weights) Adjustment:\n",
    "\n",
    "In a machine learning model, parameters (weights) are adjusted to minimize the cost function and improve the model's performance.\n",
    "Adjusting the parameters involves finding the direction and magnitude of change that reduces the cost.\n",
    "Gradient:\n",
    "\n",
    "The gradient is a vector that points in the direction of the steepest increase in the function.\n",
    "The negative gradient points in the direction of the steepest decrease, and this is the direction we want to move to minimize the cost.\n",
    "Learning Rate:\n",
    "\n",
    "The learning rate is a hyperparameter that determines the size of the steps taken during each iteration of gradient descent.\n",
    "It influences the convergence speed and the stability of the algorithm.\n",
    "Steps of Gradient Descent:\n",
    "\n",
    "Initialize Parameters:\n",
    "\n",
    "Start with initial values for the parameters (weights).\n",
    "Calculate Gradient:\n",
    "\n",
    "Compute the gradient of the cost function with respect to each parameter.\n",
    "Update Parameters:\n",
    "\n",
    "Adjust the parameters in the direction opposite to the gradient, scaled by the learning rate.\n",
    "This step aims to move closer to the minimum of the cost function.\n",
    "Repeat:\n",
    "\n",
    "Iterate steps 2 and 3 until convergence or a predetermined number of iterations.\n",
    "Types of Gradient Descent:\n",
    "\n",
    "Batch Gradient Descent:\n",
    "\n",
    "Computes the gradient using the entire training dataset at each iteration.\n",
    "Computationally expensive for large datasets but can converge to a more accurate minimum.\n",
    "Stochastic Gradient Descent (SGD):\n",
    "\n",
    "Updates the parameters using only one randomly chosen data point at each iteration.\n",
    "Faster convergence but can be noisy.\n",
    "Mini-Batch Gradient Descent:\n",
    "\n",
    "A compromise between batch and stochastic gradient descent.\n",
    "Updates the parameters using a small random subset of the training data at each iteration.\n",
    "Use in Machine Learning:\n",
    "Gradient descent is a fundamental optimization algorithm used in various machine learning models, including linear regression, logistic regression, neural networks, and more. It is employed during the training phase to find the optimal set of parameters that minimizes the cost function and allows the model to make accurate predictions on unseen data. The learning rate and the choice of gradient descent variant (batch, stochastic, mini-batch) are important considerations that can impact the performance and convergence speed of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2818941a-0ce9-4613-97e5-ed4e43676340",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516d4c63-ccf4-490d-9d25-a9187d159bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Multiple Linear Regression Model:\n",
    "\n",
    "Multiple linear regression is an extension of simple linear regression that models the relationship between a dependent variable (\n",
    "�\n",
    "Y) and two or more independent variables (\n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ",\n",
    "…\n",
    ",\n",
    "�\n",
    "�\n",
    "X \n",
    "1\n",
    "​\n",
    " ,X \n",
    "2\n",
    "​\n",
    " ,…,X \n",
    "n\n",
    "​\n",
    " ). The model assumes a linear relationship and aims to find the best-fitting linear equation that predicts the dependent variable based on the values of the independent variables. The general form of the multiple linear regression equation is:\n",
    "\n",
    "�\n",
    "=\n",
    "�\n",
    "0\n",
    "+\n",
    "�\n",
    "1\n",
    "�\n",
    "1\n",
    "+\n",
    "�\n",
    "2\n",
    "�\n",
    "2\n",
    "+\n",
    "…\n",
    "+\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "Y=b \n",
    "0\n",
    "​\n",
    " +b \n",
    "1\n",
    "​\n",
    " X \n",
    "1\n",
    "​\n",
    " +b \n",
    "2\n",
    "​\n",
    " X \n",
    "2\n",
    "​\n",
    " +…+b \n",
    "n\n",
    "​\n",
    " X \n",
    "n\n",
    "​\n",
    " +ε\n",
    "\n",
    "�\n",
    "Y is the dependent variable.\n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ",\n",
    "…\n",
    ",\n",
    "�\n",
    "�\n",
    "X \n",
    "1\n",
    "​\n",
    " ,X \n",
    "2\n",
    "​\n",
    " ,…,X \n",
    "n\n",
    "​\n",
    "  are the independent variables.\n",
    "�\n",
    "0\n",
    "b \n",
    "0\n",
    "​\n",
    "  is the y-intercept.\n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ",\n",
    "…\n",
    ",\n",
    "�\n",
    "�\n",
    "b \n",
    "1\n",
    "​\n",
    " ,b \n",
    "2\n",
    "​\n",
    " ,…,b \n",
    "n\n",
    "​\n",
    "  are the slopes for the corresponding independent variables.\n",
    "�\n",
    "ε is the error term, representing unobserved factors that affect \n",
    "�\n",
    "Y but are not included in the model.\n",
    "Differences from Simple Linear Regression:\n",
    "\n",
    "Number of Independent Variables:\n",
    "\n",
    "In simple linear regression, there is only one independent variable (\n",
    "�\n",
    "X), whereas multiple linear regression involves two or more independent variables (\n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ",\n",
    "…\n",
    ",\n",
    "�\n",
    "�\n",
    "X \n",
    "1\n",
    "​\n",
    " ,X \n",
    "2\n",
    "​\n",
    " ,…,X \n",
    "n\n",
    "​\n",
    " ).\n",
    "Equation Complexity:\n",
    "\n",
    "The equation for multiple linear regression is more complex than that of simple linear regression. It includes multiple slopes (\n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ",\n",
    "…\n",
    ",\n",
    "�\n",
    "�\n",
    "b \n",
    "1\n",
    "​\n",
    " ,b \n",
    "2\n",
    "​\n",
    " ,…,b \n",
    "n\n",
    "​\n",
    " ) corresponding to each independent variable.\n",
    "Interpretation of Slopes:\n",
    "\n",
    "In simple linear regression, the slope represents the change in the dependent variable for a one-unit change in the single independent variable. In multiple linear regression, each slope (\n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ",\n",
    "…\n",
    ",\n",
    "�\n",
    "�\n",
    "b \n",
    "1\n",
    "​\n",
    " ,b \n",
    "2\n",
    "​\n",
    " ,…,b \n",
    "n\n",
    "​\n",
    " ) represents the change in the dependent variable for a one-unit change in the corresponding independent variable, assuming all other variables are held constant.\n",
    "Model Complexity:\n",
    "\n",
    "Multiple linear regression can capture more complex relationships between the dependent variable and multiple predictors, allowing for the consideration of interactions and simultaneous influences of multiple factors.\n",
    "Visualization:\n",
    "\n",
    "Simple linear regression can be visualized in a two-dimensional scatter plot, where the relationship between the dependent and independent variables is represented by a straight line. Multiple linear regression involves more than two variables, making visualization challenging. Diagnostic plots, residual analysis, and statistical tests are often used to assess the model fit.\n",
    "Collinearity:\n",
    "\n",
    "Multiple linear regression introduces the concept of multicollinearity, where independent variables are correlated with each other. High multicollinearity can affect the stability and interpretability of the regression coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cf1fa0-38b5-4eaf-8dfd-d51c298808eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5343df-c756-41fd-9498-84fceeb92ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Multicollinearity in Multiple Linear Regression:\n",
    "\n",
    "Multicollinearity is a phenomenon in multiple linear regression where two or more independent variables in the model are highly correlated, making it challenging to distinguish the individual effects of each variable on the dependent variable. It can lead to issues such as inflated standard errors, making coefficient estimates unstable and less reliable. Multicollinearity does not impact the overall predictive power of the model but affects the precision of individual coefficients.\n",
    "\n",
    "Key Points:\n",
    "\n",
    "High Correlation:\n",
    "\n",
    "Multicollinearity is characterized by a high correlation between two or more independent variables. This high correlation can be problematic when estimating the coefficients of the regression model.\n",
    "Effects on Coefficients:\n",
    "\n",
    "In the presence of multicollinearity, it becomes difficult to isolate the unique contribution of each independent variable, as the effects are confounded due to their interdependence.\n",
    "Inflated Standard Errors:\n",
    "\n",
    "Multicollinearity leads to inflated standard errors of the regression coefficients. High standard errors imply greater uncertainty in the estimates.\n",
    "Unstable Coefficients:\n",
    "\n",
    "Coefficient estimates become unstable, meaning that small changes in the data can result in substantial changes in the estimated coefficients.\n",
    "Detection of Multicollinearity:\n",
    "\n",
    "Correlation Matrix:\n",
    "\n",
    "Examine the correlation matrix between independent variables. High correlation coefficients (close to 1 or -1) suggest potential multicollinearity.\n",
    "Variance Inflation Factor (VIF):\n",
    "\n",
    "VIF measures how much the variance of an estimated regression coefficient increases if your predictors are correlated. A VIF greater than 10 is often considered a sign of multicollinearity.\n",
    "Tolerance:\n",
    "\n",
    "Tolerance is the reciprocal of the VIF. A low tolerance value (close to 0) indicates high multicollinearity.\n",
    "Condition Number:\n",
    "\n",
    "The condition number is another indicator of multicollinearity. A large condition number suggests multicollinearity may be a problem.\n",
    "Addressing Multicollinearity:\n",
    "\n",
    "Variable Selection:\n",
    "\n",
    "Remove redundant variables that contribute to multicollinearity. Prioritize variables with theoretical importance or higher relevance.\n",
    "Data Transformation:\n",
    "\n",
    "Transform variables to reduce multicollinearity. Techniques include centering variables (subtracting the mean), scaling (dividing by the standard deviation), or creating interaction terms.\n",
    "Principal Component Analysis (PCA):\n",
    "\n",
    "PCA can be used to transform the original variables into a set of linearly uncorrelated variables, known as principal components. This can mitigate multicollinearity.\n",
    "Ridge Regression:\n",
    "\n",
    "Ridge regression is a regularization technique that penalizes large coefficients, helping to stabilize them in the presence of multicollinearity.\n",
    "Collect More Data:\n",
    "\n",
    "Increasing the sample size can sometimes alleviate multicollinearity issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430f8672-2ea1-40db-ac8c-e6b702b55785",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3c87d4-a049-4603-a235-79966ab51639",
   "metadata": {},
   "outputs": [],
   "source": [
    "Polynomial Regression Model:\n",
    "\n",
    "Polynomial regression is an extension of linear regression that allows for modeling non-linear relationships between the dependent variable and one or more independent variables. In polynomial regression, the relationship is described by a polynomial equation of a specified degree. The general form of a polynomial regression equation with one independent variable is:\n",
    "\n",
    "�\n",
    "=\n",
    "�\n",
    "0\n",
    "+\n",
    "�\n",
    "1\n",
    "�\n",
    "+\n",
    "�\n",
    "2\n",
    "�\n",
    "2\n",
    "+\n",
    "…\n",
    "+\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "Y=b \n",
    "0\n",
    "​\n",
    " +b \n",
    "1\n",
    "​\n",
    " X+b \n",
    "2\n",
    "​\n",
    " X \n",
    "2\n",
    " +…+b \n",
    "n\n",
    "​\n",
    " X \n",
    "n\n",
    " +ε\n",
    "\n",
    "�\n",
    "Y is the dependent variable.\n",
    "�\n",
    "X is the independent variable.\n",
    "�\n",
    "0\n",
    ",\n",
    "�\n",
    "1\n",
    ",\n",
    "…\n",
    ",\n",
    "�\n",
    "�\n",
    "b \n",
    "0\n",
    "​\n",
    " ,b \n",
    "1\n",
    "​\n",
    " ,…,b \n",
    "n\n",
    "​\n",
    "  are the coefficients.\n",
    "�\n",
    "n is the degree of the polynomial.\n",
    "�\n",
    "ε is the error term.\n",
    "Polynomial regression allows for more flexibility in capturing non-linear patterns in the data by including higher-degree terms of the independent variable.\n",
    "\n",
    "Differences from Linear Regression:\n",
    "\n",
    "Equation Form:\n",
    "\n",
    "In linear regression, the relationship between the dependent and independent variables is described by a straight line, and the equation is linear. In polynomial regression, the relationship is described by a polynomial equation, introducing higher-degree terms.\n",
    "Nature of Relationship:\n",
    "\n",
    "Linear regression assumes a linear relationship between variables, meaning the change in the dependent variable is constant for a one-unit change in the independent variable. Polynomial regression can capture non-linear relationships, allowing for curves, peaks, and valleys.\n",
    "Degree of Flexibility:\n",
    "\n",
    "Polynomial regression is more flexible in capturing complex patterns in the data, especially when the relationship is not strictly linear. By increasing the degree of the polynomial, the model can fit more intricate shapes.\n",
    "Overfitting Risk:\n",
    "\n",
    "While polynomial regression provides more flexibility, there is a risk of overfitting the model to the noise in the data, especially with a high-degree polynomial. Overfitting occurs when the model fits the training data too closely and performs poorly on new, unseen data.\n",
    "Interpretability:\n",
    "\n",
    "Linear regression coefficients have straightforward interpretations: the slope represents the change in the dependent variable for a one-unit change in the independent variable. In polynomial regression with higher degrees, the interpretation becomes more complex.\n",
    "Example:\n",
    "Consider a scenario where you want to predict a person's weight (\n",
    "�\n",
    "Y) based on their height (\n",
    "�\n",
    "X). In linear regression, the equation might be:\n",
    "\n",
    "Weight\n",
    "=\n",
    "�\n",
    "0\n",
    "+\n",
    "�\n",
    "1\n",
    "×\n",
    "Height\n",
    "+\n",
    "�\n",
    "Weight=b \n",
    "0\n",
    "​\n",
    " +b \n",
    "1\n",
    "​\n",
    " ×Height+ε\n",
    "\n",
    "In polynomial regression, you could use a quadratic (degree 2) polynomial:\n",
    "\n",
    "Weight\n",
    "=\n",
    "�\n",
    "0\n",
    "+\n",
    "�\n",
    "1\n",
    "×\n",
    "Height\n",
    "+\n",
    "�\n",
    "2\n",
    "×\n",
    "(\n",
    "Height\n",
    ")\n",
    "2\n",
    "+\n",
    "�\n",
    "Weight=b \n",
    "0\n",
    "​\n",
    " +b \n",
    "1\n",
    "​\n",
    " ×Height+b \n",
    "2\n",
    "​\n",
    " ×(Height) \n",
    "2\n",
    " +ε\n",
    "\n",
    "This allows the model to capture a non-linear relationship between height and weight, potentially accounting for factors like growth spurts.\n",
    "\n",
    "It's important to note that while polynomial regression can be powerful for capturing non-linear relationships, careful consideration of the model complexity and the risk of overfitting is necessary. Regularization techniques and model evaluation on new data can help address overfitting concerns.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a554a4-613b-40ec-ab29-f8289aa26be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc5e65c-c94a-419d-a9cb-ee1bd180aa3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8126ff37-7fdb-4856-bb14-cc5760408703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db13ff2-0d81-484e-b5fd-929653be6dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54586bc-45e8-433c-a33a-e51ebe033bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
